---
title: "5 Levels of Optimizing Tokenizer Training with Simple Python Code"
weight: 1
mermaid: true
math: true
draft: false
date: 2025-04-01
categories: ["technology", "Hugo"]
tags: ["static site generator", "tutorial", "documentation"]
---

# Introduction

Tokenization of text from scratch typically involves three steps: Pretokenization, Training a tokenizer, Writing an encoder-decoder pair which uses the learned vocab and merges files generated by the tokenizer. Typically, the most time and memory consuming step is the training of the tokenizer. In this post, I will show five levels of optimization in the training of a BPE tokenizer in 5 levels, starting from a simple Python code and ending with an performant rust implementation.

# Context

# Optimizations

## Merge updates

## Sorting in intervals

## Processing Subsets

## Parallelization

## Porting to Rust

# Conclusion
