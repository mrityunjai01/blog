<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Tech on Collection of Short Articles on ML</title>
    <link>/</link>
    <description>Recent content in Blog on Tech on Collection of Short Articles on ML</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Parametrizing Decision Trees</title>
      <link>/docs/param_dt/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>/docs/param_dt/</guid>
      <description></description>
    </item>
    <item>
      <title>Parametrizing Decision Trees</title>
      <link>/docs/search/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>/docs/search/</guid>
      <description>&lt;p&gt;References:&#xA;[https://learn.microsoft.com/en-us/azure/search/search-relevance-overview]&#xA;[https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking]&lt;/p&gt;</description>
    </item>
    <item>
      <title>Can Machines Search for Omitted Variables</title>
      <link>/docs/omitted_vars/</link>
      <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/docs/omitted_vars/</guid>
      <description>&lt;!-- markdownlint-disable MD025 MD013 --&gt;&#xA;&lt;p&gt;There&amp;rsquo;s some consensus that a lot of methods which are used to detect omitted variables don&amp;rsquo;t worka well if the omitted feature is not correlated with the covariates. This causes a problem when trying to detect possible omission of variables. In this post, I will show that even if we know that there is omission, we can&amp;rsquo;t identify omitted variables by only using analytical methods. We need to bring practical information in to ascertain causality, we&amp;rsquo;re otherwise left with a set of possible correlated variables.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Snooping in Technical Trading</title>
      <link>/docs/technical_trading/</link>
      <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/docs/technical_trading/</guid>
      <description>&lt;!-- markdownlint-disable MD025 MD013 --&gt;&#xA;&lt;h1 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Technical Trading is defined by the exclusive use of data from past prices and volumes to make trading decisions. It has been widely contested that technical trading doesn&amp;rsquo;t outperform major benchmarks owing to the weak form of the Efficient Market Hypothesis (EMH). However, there are many who believe that technical trading can be profitable, especially when combined with other methods such as fundamental analysis or machine learning. This is especially true when some strategies outperform benchmarks in back-test, leading researchers to claim that those strategies are profitable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum Likelihood Estimation for Neural Networks</title>
      <link>/docs/mle_nn/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/docs/mle_nn/</guid>
      <description>&lt;!-- markdownlint-disable MD025 MD013 --&gt;&#xA;&lt;h1 id=&#34;maximum-likelihood-estimation-for-neural-networks&#34;&gt;&#xA;  Maximum Likelihood Estimation for Neural Networks&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#maximum-likelihood-estimation-for-neural-networks&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Maximum Likelihood Estimation is a concept that allows us to estimate parameters of a statistical model. Most models can be thought of as random variables which are functions of some parameters (weights and biases). These weights and biases, combined with some random draws from fundamental distributions (e.g., Gaussian, Bernoulli), can be said to generate data. The combination of weights and draws from random distributions is called a model which is supposed to describe the relationship between inputs \(x_i\) and outputs \(y_i\). The goal of MLE is to find the parameters that maximize the likelihood of observing the given data \(x_i\) and \(y_i\), where \(i\) comes from an indexing set.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SPAN</title>
      <link>/docs/span/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/docs/span/</guid>
      <description>&lt;!-- markdownlint-disable MD025 MD013 --&gt;</description>
    </item>
    <item>
      <title></title>
      <link>/docs/bayes_nn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/bayes_nn/</guid>
      <description></description>
    </item>
  </channel>
</rss>
