<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>A Short Collection of Articles on Tech, ML and Statistics on Collection of Short Articles on ML</title>
    <link>/blog/</link>
    <description>Recent content in A Short Collection of Articles on Tech, ML and Statistics on Collection of Short Articles on ML</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 13 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Zero-shot Retrieval With Priors</title>
      <link>/blog/docs/ret_1/</link>
      <pubDate>Sun, 13 Jul 2025 00:00:00 +0000</pubDate>
      <guid>/blog/docs/ret_1/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Retrieval refers to searching for a response to a query from a large corpus of documents. In the context of information retrieval, it is often necessary to retrieve relevant documents or passages from a large collection based on a given query. This process is typically performed using various retrieval models that rank documents based on their relevance to the query.&lt;/p&gt;&#xA;&lt;p&gt;Zero-shot retrieval refers to retrieval without any prior training. This might seem a daunting task at the outset, but it has been observed that simple sparse models like BM25 which use lexical similarity have been shown to have promising generalization capability. One way to measure this capability is to evaluate it on benchmarks such as BEIR.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parametrizing Decision Trees</title>
      <link>/blog/docs/param_dt/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>/blog/docs/param_dt/</guid>
      <description></description>
    </item>
    <item>
      <title>Parametrizing Decision Trees</title>
      <link>/blog/docs/search/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>/blog/docs/search/</guid>
      <description>&lt;p&gt;References:&#xA;[https://learn.microsoft.com/en-us/azure/search/search-relevance-overview]&#xA;[https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking]&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reddit Astroturfing: A Bayesian Approach</title>
      <link>/blog/docs/reddit_astroturfing/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>/blog/docs/reddit_astroturfing/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;We&amp;rsquo;ve all come across reddit posts with controversial or opinionated claims and are surprised by the number of supporting comments they receive. At times, this is merely a function of genuine support for contrarian viewpoints. However, if I come across the same small subset of users promoting a particular narrative, I sometimes begin to question if there&amp;rsquo;s a bit of a systematic component to it. There is some chance that a group of users collude to write group-affirming comments, perhaps supported or controlled by a single entity. This is usually suspicious if there&amp;rsquo;s very little activity from such astroturfing users apart from such posts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Can Machines Search for Omitted Variables</title>
      <link>/blog/docs/omitted_vars/</link>
      <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/blog/docs/omitted_vars/</guid>
      <description>&lt;!-- markdownlint-disable MD025 MD013 --&gt;&#xA;&lt;p&gt;There&amp;rsquo;s some consensus that a lot of methods which are used to detect omitted variables don&amp;rsquo;t worka well if the omitted feature is not correlated with the covariates. This causes a problem when trying to detect possible omission of variables. In this post, I will show that even if we know that there is omission, we can&amp;rsquo;t identify omitted variables by only using analytical methods. We need to bring practical information in to ascertain causality, we&amp;rsquo;re otherwise left with a set of possible correlated variables.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Snooping in Technical Trading</title>
      <link>/blog/docs/technical_trading/</link>
      <pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/blog/docs/technical_trading/</guid>
      <description>&lt;!-- markdownlint-disable MD025 MD013 --&gt;&#xA;&lt;h1 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Technical Trading is defined by the exclusive use of data from past prices and volumes to make trading decisions. It has been widely contested that technical trading doesn&amp;rsquo;t outperform major benchmarks owing to the weak form of the Efficient Market Hypothesis (EMH). However, there are many who believe that technical trading can be profitable, especially when combined with other methods such as fundamental analysis or machine learning. This is especially true when some strategies outperform benchmarks in back-test, leading researchers to claim that those strategies are profitable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum Likelihood Estimation for Neural Networks</title>
      <link>/blog/docs/mle_nn/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/blog/docs/mle_nn/</guid>
      <description>&lt;!-- markdownlint-disable MD025 MD013 --&gt;&#xA;&lt;h1 id=&#34;maximum-likelihood-estimation-for-neural-networks&#34;&gt;&#xA;  Maximum Likelihood Estimation for Neural Networks&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#maximum-likelihood-estimation-for-neural-networks&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Maximum Likelihood Estimation is a concept that allows us to estimate parameters of a statistical model. Most models can be thought of as random variables which are functions of some parameters (weights and biases). These weights and biases, combined with some random draws from fundamental distributions (e.g., Gaussian, Bernoulli), can be said to generate data. The combination of weights and draws from random distributions is called a model which is supposed to describe the relationship between inputs \(x_i\) and outputs \(y_i\). The goal of MLE is to find the parameters that maximize the likelihood of observing the given data \(x_i\) and \(y_i\), where \(i\) comes from an indexing set.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SPAN</title>
      <link>/blog/docs/span/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/blog/docs/span/</guid>
      <description>&lt;!-- markdownlint-disable MD025 MD013 --&gt;</description>
    </item>
    <item>
      <title></title>
      <link>/blog/docs/bayes_nn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/blog/docs/bayes_nn/</guid>
      <description></description>
    </item>
  </channel>
</rss>
