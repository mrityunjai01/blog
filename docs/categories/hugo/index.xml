<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hugo on Collection of Short Articles on ML</title>
    <link>/categories/hugo/</link>
    <description>Recent content in Hugo on Collection of Short Articles on ML</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/hugo/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>5 Levels of Optimizing Tokenizer Training with Simple Python Code</title>
      <link>/docs/bpe_tokenizer_training/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/docs/bpe_tokenizer_training/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Tokenization of text from scratch typically involves three steps: Pretokenization, Training a tokenizer, Writing an encoder-decoder pair which uses the learned vocab and merges files generated by the tokenizer. Typically, the most time and memory consuming step is the training of the tokenizer. In this post, I will show five levels of optimization in the training of a BPE tokenizer in 5 levels, starting from a simple Python code and ending with an performant rust implementation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum Likelihood Estimation for Neural Networks</title>
      <link>/docs/mle_nn/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/docs/mle_nn/</guid>
      <description>&lt;!-- markdownlint-disable MD025 MD013 --&gt;&#xA;&lt;h1 id=&#34;maximum-likelihood-estimation-for-neural-networks&#34;&gt;&#xA;  Maximum Likelihood Estimation for Neural Networks&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#maximum-likelihood-estimation-for-neural-networks&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;In this article, I&amp;rsquo;ll talk about the issues that Maximum Likelihood Estimation faces with Neural Networks. Maximum Likelihood Estimation is a concept that allows us to estimate parameters of a statistical model. Most models can be thought of as random variables which are functions of some parameters (weights and biases). These weights and biases, combined with some random draws from fundamental distributions (e.g., Gaussian, Bernoulli), can be said to generate data. The combination of weights and draws from random distributions is called a model which is supposed to describe the relationship between inputs \(x_i\) and outputs \(y_i\). The goal of MLE is to find the parameters that maximize the likelihood of observing the given data \(x_i\) and \(y_i\), where \(i\) comes from an indexing set.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
