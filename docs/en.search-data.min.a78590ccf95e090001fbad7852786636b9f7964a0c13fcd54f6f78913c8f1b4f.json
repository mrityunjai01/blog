[{"id":0,"href":"/blog/docs/ret_1/","title":"Zero-shot Retrieval With Priors","section":"Blog on Tech","content":" Introduction # Retrieval refers to searching for a response to a query from a large corpus of documents. In the context of information retrieval, it is often necessary to retrieve relevant documents or passages from a large collection based on a given query. This process is typically performed using various retrieval models that rank documents based on their relevance to the query.\nZero-shot retrieval refers to retrieval without any prior training. This might seem a daunting task at the outset, but it has been observed that simple sparse models like BM25 which use lexical similarity have been shown to have promising generalization capability. One way to measure this capability is to evaluate it on benchmarks such as BEIR.\nThe BEIR benchmark, introduced by Thakur et al is designed to evaluate information retrieval systems across diverse combinations of tasks and domains. It was designed to focus on the “zero-shot” retrieval setting, i.e., evaluation on tasks and domains without any training data or supervision signals.\nReferences # [1] Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard. Ehsan Kamalloo, Nandan Thakur, Carlos Lassance, Xueguang Ma, Jheng-Hong Yang, \u0026amp; Jimmy Lin. (2023)\n"},{"id":1,"href":"/blog/docs/search/","title":"Parametrizing Decision Trees","section":"Blog on Tech","content":"References: [https://learn.microsoft.com/en-us/azure/search/search-relevance-overview] [https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking]\n"},{"id":2,"href":"/blog/docs/reddit_astroturfing/","title":"Reddit Astroturfing: A Bayesian Approach","section":"Blog on Tech","content":" Introduction # We\u0026rsquo;ve all come across reddit posts with controversial or opinionated claims and are surprised by the number of supporting comments they receive. At times, this is merely a function of genuine support for contrarian viewpoints. However, if I come across the same small subset of users promoting a particular narrative, I sometimes begin to question if there\u0026rsquo;s a bit of a systematic component to it. There is some chance that a group of users collude to write group-affirming comments, perhaps supported or controlled by a single entity. This is usually suspicious if there\u0026rsquo;s very little activity from such astroturfing users apart from such posts.\nIn this post, I aim to describe a Bayesian Inference approach which handles the problem of detecting such coordinated commenting behavior on reddit posts. The model is designed to differentiate between genuine user engagement and potential astroturfing activity, while accounting for the natural variation in post popularity and user commenting behavior.\nWe have to be careful to differentiate this from other non-offending posts which receive a large number of broadly interested community members, which might also coincidentally receive comments from astroturfing users. There needs to be a way to properly measure the certainty with which we can claim that a given set of users is astroturfing. Not just that, it\u0026rsquo;d be helpful if we could get all subsets of astroturfing users.\nA common approach used to detect such manipulative activity is to use timestamp data to filter out manipulative activity which took place in a short span of time. This is particularly useful when there\u0026rsquo;s a long persisting interaction with content, such as in Online Shopping reviews, or Yelp reviews. However, this is not effective and misses a lot of activity on social media comment sections which anyways happens in a short span of time.\nIntuition # There are two things we need to take care of:\nThere are certain posts with high participation from the community. On such posts, we might observe a lot of coincidental simultaneous involvement from unrelated users. Our model should be able to learn this probabilistic behaviour of comment-post interaction. This also holds true for posts with low participation, where simultaneous involvement is more suspicious than in the case of high participation posts. There are some commenters who comment on a lot of posts, and some who comment on very few. We need to account for this in our model, so that we can differentiate between a users who are colluding and ones who are indiscriminately commenting on posts. Reasoning # This hierarchical Bayesian model addresses the challenge of detecting coordinated commenting behavior while accounting for natural variation in both post popularity and user activity levels. Key Design Rationale Post-Specific Effects (alpha): The model incorporates post-specific logit intercepts to capture the inherent \u0026ldquo;attractiveness\u0026rdquo; or engagement level of individual posts. High-participation posts naturally generate more simultaneous comments from unrelated users, making coincidental overlap common and less suspicious. Conversely, low-participation posts have lower baseline engagement, so simultaneous commenting becomes more indicative of potential coordination rather than organic interest. User-Specific Effects (beta): Commentor-specific parameters account for individual propensities to comment across the platform. Some users are naturally prolific commenters who engage with many posts indiscriminately, while others are selective participants. By modeling these baseline tendencies, the model can distinguish between users who comment together frequently due to high individual activity rates versus those whose joint participation exceeds what would be expected from their individual behaviors alone. Hierarchical Structure: The inclusion of additional commenting data (y_rem) through the hierarchical prior prevents overfitting to the specific posts under investigation. This allows the model to learn realistic baseline commenting probabilities from a broader sample of user behavior, providing better context for evaluating whether observed co-commenting patterns are suspicious or within normal bounds.\nProgram # Here\u0026rsquo;s the stan program we use for modelling comment behaviour.\ndata { int\u0026lt;lower=1\u0026gt; N; int\u0026lt;lower=1\u0026gt; S; int\u0026lt;lower=1\u0026gt; R; array[S, N] int\u0026lt;lower=0,upper=1\u0026gt; y; array[S] int\u0026lt;lower=0\u0026gt; y_rem; } parameters { vector[N] alpha; // item-specific logit intercepts vector[S] beta; // subset-specific effects real mu_alpha; // hyperprior mean for item effects real\u0026lt;lower=0\u0026gt; sigma_alpha; // hyperprior sd for item effects real mu_beta; real\u0026lt;lower=0\u0026gt; sigma_beta; real alpha_0; real mu_alpha_0; real\u0026lt;lower=0\u0026gt; sigma_alpha_0; } model { // Hyperpriors mu_alpha ~ normal(0, 1); sigma_alpha ~ exponential(1); mu_beta ~ normal(0, 1); sigma_beta ~ exponential(1); mu_alpha_0 ~ normal(0, 1); sigma_alpha_0 ~ exponential(1); // Hierarchical priors alpha_0 ~ normal(mu_alpha_0, sigma_alpha_0); alpha ~ normal(alpha_0, sigma_alpha); beta ~ normal(mu_beta, sigma_beta); // Likelihood for (s in 1:S) { for (n in 1:N) { y[s,n] ~ bernoulli_logit(alpha[n] + beta[s]); } y_rem[s] ~ binomial(R, inv_logit(alpha_0 + beta[s])); } } This Stan program implements a hierarchical Bayesian model to analyze commenting patterns between users and posts, incorporating both observed and unobserved commenting behavior.\nModel Structure # The model assumes that the probability of commentor s commenting on post n follows a Bernoulli distribution with logit-linear predictors combining post-specific and commentor-specific effects.\nData Variables # N (int\u0026lt;lower=1\u0026gt;): Number of posts in the observed dataset S (int\u0026lt;lower=1\u0026gt;): Number of commentors being studied R (int\u0026lt;lower=1\u0026gt;): Number of additional posts not in the main dataset (used for hierarchical modeling) y (array[S, N] int\u0026lt;lower=0,upper=1\u0026gt;): Binary comment matrix where y[s,n] = 1 if commentor s commented on post n, 0 otherwise y_rem (array[S] int\u0026lt;lower=0\u0026gt;): Count of comments each commentor s made on the R additional posts Parameters # alpha (vector[N]): Post-specific logit intercepts representing each post\u0026rsquo;s baseline propensity to receive comments beta (vector[S]): Commentor-specific logit effects representing each commentor\u0026rsquo;s baseline propensity to comment mu_alpha (real): Population mean for post effects (unused in current specification) sigma_alpha (real\u0026lt;lower=0\u0026gt;): Population standard deviation for post effects mu_beta (real): Population mean for commentor effects sigma_beta (real\u0026lt;lower=0\u0026gt;): Population standard deviation for commentor effects alpha_0 (real): Baseline post effect for the additional R posts mu_alpha_0 (real): Prior mean for baseline post effect sigma_alpha_0 (real\u0026lt;lower=0\u0026gt;): Prior standard deviation for baseline post effect Prior Distributions # mu_alpha ~ Normal(0, 1) sigma_alpha ~ Exponential(1) mu_beta ~ Normal(0, 1) sigma_beta ~ Exponential(1) mu_alpha_0 ~ Normal(0, 1) sigma_alpha_0 ~ Exponential(1) alpha_0 ~ Normal(mu_alpha_0, sigma_alpha_0) alpha[n] ~ Normal(alpha_0, sigma_alpha) for n = 1, \u0026hellip;, N beta[s] ~ Normal(mu_beta, sigma_beta) for s = 1, \u0026hellip;, S Likelihood Distributions # y[s,n] ~ Bernoulli_logit(alpha[n] + beta[s]) for s = 1, \u0026hellip;, S and n = 1, \u0026hellip;, N y_rem[s] ~ Binomial(R, inv_logit(alpha_0 + beta[s])) for s = 1, \u0026hellip;, S The hierarchical structure allows information from the additional posts (y_rem) to inform the prior distribution for the observed post effects (alpha) through the shared baseline alpha_0.\n"},{"id":3,"href":"/blog/docs/omitted_vars/","title":"Can Machines Search for Omitted Variables","section":"Blog on Tech","content":" There\u0026rsquo;s some consensus that a lot of methods which are used to detect omitted variables don\u0026rsquo;t worka well if the omitted feature is not correlated with the covariates. This causes a problem when trying to detect possible omission of variables. In this post, I will show that even if we know that there is omission, we can\u0026rsquo;t identify omitted variables by only using analytical methods. We need to bring practical information in to ascertain causality, we\u0026rsquo;re otherwise left with a set of possible correlated variables.\nI\u0026rsquo;ll first summarize detection methods for omitted variables in a multiple regression setting.\n"},{"id":4,"href":"/blog/docs/technical_trading/","title":"Snooping in Technical Trading","section":"Blog on Tech","content":" Introduction # Technical Trading is defined by the exclusive use of data from past prices and volumes to make trading decisions. It has been widely contested that technical trading doesn\u0026rsquo;t outperform major benchmarks owing to the weak form of the Efficient Market Hypothesis (EMH). However, there are many who believe that technical trading can be profitable, especially when combined with other methods such as fundamental analysis or machine learning. This is especially true when some strategies outperform benchmarks in back-test, leading researchers to claim that those strategies are profitable.\nIn this post, I will show that there are certain conditions on the time series price signal which can be exploited to make profitable trades. I will also show how it is possible to detect data snooping when selecting a performant strategy. I will also show that the profitability of technical trading is not necessarily due to the exploitation of market inefficiencies, but rather due to the exploitation of the time series properties of the price signal itself. This means that I to restrict the universe of my analysis to technical trading strategies which only use past prices as inputs to trading decisions. This excludes most strategies which use fundamental data or even data from other assets, as is the case in Statistical Arbitrage strategies.\n"},{"id":5,"href":"/blog/docs/mle_nn/","title":"Maximum Likelihood Estimation for Neural Networks","section":"Blog on Tech","content":" Maximum Likelihood Estimation for Neural Networks # Introduction # Maximum Likelihood Estimation is a concept that allows us to estimate parameters of a statistical model. Most models can be thought of as random variables which are functions of some parameters (weights and biases). These weights and biases, combined with some random draws from fundamental distributions (e.g., Gaussian, Bernoulli), can be said to generate data. The combination of weights and draws from random distributions is called a model which is supposed to describe the relationship between inputs \\(x_i\\) and outputs \\(y_i\\). The goal of MLE is to find the parameters that maximize the likelihood of observing the given data \\(x_i\\) and \\(y_i\\), where \\(i\\) comes from an indexing set.\nMLE not only provides estimates of the parameters (an estimate is a function of data observations \\(x_i\\) and \\(y_i\\), which is said to be equal in expectation to some function of \u0026ldquo;actual\u0026rdquo; population parameters), but also gives us some nice properties of the model. MLE doesn\u0026rsquo;t offer any generalizability guarantees, neither does a well fit NN. I aim to show that even a generalizable NN doesn\u0026rsquo;t have consistent parameters.\nConsistency: As the sample size approaches infinity, the MLE converges in probability to the true parameter value. This means that with sufficiently large samples, the estimator will be arbitrarily close to the actual parameter. Asymptotic Normality Asymptotic Efficiency For these desirable properties to hold, several regularity conditions must be satisfied:\nIdentifiability: Different parameter values must produce different probability distributions. The mapping from parameters to distributions must be one-to-one.\nDifferentiability, Regularity of Support, Smoothness Conditions, Finite Fisher Information, Compact Parameter Space.\nWe\u0026rsquo;ll focus on Identifiability and show that identifiability is not satisfied for even generalizable neural network architectures. We\u0026rsquo;ll try to ascertain, using an example, whether the MLE is consistent.\nA Simple Neural Network Example # Constructed Toy Data Analysis # I have this task at\nI construct a simple neural network with one hidden layer and one output layer. The hidden layer has three neurons, and the output layer has one neuron. The hidden layer uses the hyperbolic tangent activation function, and the output layer is linear with a sigmoid activation to model the output as a probability.\ngraph LR %% Input Layer X1[x₁\u0026lt;br/\u0026gt;Input 1] X2[x₂\u0026lt;br/\u0026gt;Input 2] %% Hidden Layer H1[\u0026#34;h₁\u0026lt;br/\u0026gt;Hidden 1: h = tanh (w₁₁ * x₁ + w₂₁ * x₂ + b₁)\u0026#34;] H2[h₂\u0026lt;br/\u0026gt;Hidden 2] H3[h₃\u0026lt;br/\u0026gt;Hidden 3] %% Output Layer Y[y\u0026lt;br/\u0026gt;Output] %% Connections with weights X1 -.-\u0026gt;|\u0026#34;[w₁₁]\u0026#34;| H1 X1 -.-\u0026gt;|\u0026#34;[w₁₂]\u0026#34;| H2 X1 -.-\u0026gt; H3 X2 -.-\u0026gt;|\u0026#34;[w₂₁]\u0026#34;| H1 X2 -.-\u0026gt;|\u0026#34;[w₂₂]\u0026#34;| H2 X2 -.-\u0026gt; H3 H1 -.-\u0026gt;|\u0026#34;[v₁]\u0026#34;| Y H2 -.-\u0026gt; Y H3 -.-\u0026gt; Y %% Styling class X1,X2 input class H1,H2,H3 hidden_nodes class Y output "},{"id":6,"href":"/blog/docs/span/","title":"SPAN","section":"Blog on Tech","content":" "},{"id":7,"href":"/blog/docs/","title":"Blog on Tech","section":"A Short Collection of Articles on Tech, ML and Statistics","content":" Intro # I created this blog to share my thoughts on some topics I find interesting.\n"}]