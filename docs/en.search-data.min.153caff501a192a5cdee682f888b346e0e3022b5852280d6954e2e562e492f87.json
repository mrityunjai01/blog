[{"id":0,"href":"/docs/ret_1/","title":"Retrieval With Priors","section":"Docs","content":" Introduction # The BEIR benchmark, introduced by Thakur et al . , is designed to evaluate information retrieval systems across diverse combi- nations of tasks and domains. It was designed to focus on the “zero-shot” retrieval setting, i.e., evaluation on tasks and domains without any training data or supervision signals.\nReferences # Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard. Ehsan Kamalloo, Nandan Thakur, Carlos Lassance, Xueguang Ma, Jheng-Hong Yang, \u0026amp; Jimmy Lin. (2023). In APA style.\n"},{"id":1,"href":"/docs/search/","title":"Parametrizing Decision Trees","section":"Docs","content":"References: [https://learn.microsoft.com/en-us/azure/search/search-relevance-overview] [https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking]\n"},{"id":2,"href":"/docs/omitted_vars/","title":"Can Machines Search for Omitted Variables","section":"Docs","content":" There\u0026rsquo;s some consensus that a lot of methods which are used to detect omitted variables don\u0026rsquo;t worka well if the omitted feature is not correlated with the covariates. This causes a problem when trying to detect possible omission of variables. In this post, I will show that even if we know that there is omission, we can\u0026rsquo;t identify omitted variables by only using analytical methods. We need to bring practical information in to ascertain causality, we\u0026rsquo;re otherwise left with a set of possible correlated variables.\nI\u0026rsquo;ll first summarize detection methods for omitted variables in a multiple regression setting.\n"},{"id":3,"href":"/docs/technical_trading/","title":"Snooping in Technical Trading","section":"Docs","content":" Introduction # Technical Trading is defined by the exclusive use of data from past prices and volumes to make trading decisions. It has been widely contested that technical trading doesn\u0026rsquo;t outperform major benchmarks owing to the weak form of the Efficient Market Hypothesis (EMH). However, there are many who believe that technical trading can be profitable, especially when combined with other methods such as fundamental analysis or machine learning. This is especially true when some strategies outperform benchmarks in back-test, leading researchers to claim that those strategies are profitable.\nIn this post, I will show that there are certain conditions on the time series price signal which can be exploited to make profitable trades. I will also show how it is possible to detect data snooping when selecting a performant strategy. I will also show that the profitability of technical trading is not necessarily due to the exploitation of market inefficiencies, but rather due to the exploitation of the time series properties of the price signal itself. This means that I to restrict the universe of my analysis to technical trading strategies which only use past prices as inputs to trading decisions. This excludes most strategies which use fundamental data or even data from other assets, as is the case in Statistical Arbitrage strategies.\n"},{"id":4,"href":"/docs/mle_nn/","title":"Maximum Likelihood Estimation for Neural Networks","section":"Docs","content":" Maximum Likelihood Estimation for Neural Networks # Introduction # Maximum Likelihood Estimation is a concept that allows us to estimate parameters of a statistical model. Most models can be thought of as random variables which are functions of some parameters (weights and biases). These weights and biases, combined with some random draws from fundamental distributions (e.g., Gaussian, Bernoulli), can be said to generate data. The combination of weights and draws from random distributions is called a model which is supposed to describe the relationship between inputs \\(x_i\\) and outputs \\(y_i\\). The goal of MLE is to find the parameters that maximize the likelihood of observing the given data \\(x_i\\) and \\(y_i\\), where \\(i\\) comes from an indexing set.\nMLE not only provides estimates of the parameters (an estimate is a function of data observations \\(x_i\\) and \\(y_i\\), which is said to be equal in expectation to some function of \u0026ldquo;actual\u0026rdquo; population parameters), but also gives us some nice properties of the model. MLE doesn\u0026rsquo;t offer any generalizability guarantees, neither does a well fit NN. I aim to show that even a generalizable NN doesn\u0026rsquo;t have consistent parameters.\nConsistency: As the sample size approaches infinity, the MLE converges in probability to the true parameter value. This means that with sufficiently large samples, the estimator will be arbitrarily close to the actual parameter. Asymptotic Normality Asymptotic Efficiency For these desirable properties to hold, several regularity conditions must be satisfied:\nIdentifiability: Different parameter values must produce different probability distributions. The mapping from parameters to distributions must be one-to-one.\nDifferentiability, Regularity of Support, Smoothness Conditions, Finite Fisher Information, Compact Parameter Space.\nWe\u0026rsquo;ll focus on Identifiability and show that identifiability is not satisfied for even generalizable neural network architectures. We\u0026rsquo;ll try to ascertain, using an example, whether the MLE is consistent.\nA Simple Neural Network Example # Constructed Toy Data Analysis # I have this task at\nI construct a simple neural network with one hidden layer and one output layer. The hidden layer has three neurons, and the output layer has one neuron. The hidden layer uses the hyperbolic tangent activation function, and the output layer is linear with a sigmoid activation to model the output as a probability.\ngraph LR %% Input Layer X1[x₁\u0026lt;br/\u0026gt;Input 1] X2[x₂\u0026lt;br/\u0026gt;Input 2] %% Hidden Layer H1[\u0026#34;h₁\u0026lt;br/\u0026gt;Hidden 1: h = tanh (w₁₁ * x₁ + w₂₁ * x₂ + b₁)\u0026#34;] H2[h₂\u0026lt;br/\u0026gt;Hidden 2] H3[h₃\u0026lt;br/\u0026gt;Hidden 3] %% Output Layer Y[y\u0026lt;br/\u0026gt;Output] %% Connections with weights X1 -.-\u0026gt;|\u0026#34;[w₁₁]\u0026#34;| H1 X1 -.-\u0026gt;|\u0026#34;[w₁₂]\u0026#34;| H2 X1 -.-\u0026gt; H3 X2 -.-\u0026gt;|\u0026#34;[w₂₁]\u0026#34;| H1 X2 -.-\u0026gt;|\u0026#34;[w₂₂]\u0026#34;| H2 X2 -.-\u0026gt; H3 H1 -.-\u0026gt;|\u0026#34;[v₁]\u0026#34;| Y H2 -.-\u0026gt; Y H3 -.-\u0026gt; Y %% Styling class X1,X2 input class H1,H2,H3 hidden_nodes class Y output "},{"id":5,"href":"/docs/span/","title":"SPAN","section":"Docs","content":" "}]